<!DOCTYPE html lang="en">
<html>
<head>
  <meta charset="utf-8">
  <meta name="description" content="MoFlow: Self-Supervised Motion Learning for Dynamic Scene Reconstruction">
  <meta name="keywords" content="Motion, 3D Reconstruction, Dynamic Scene, Gaussian Splatting">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title><span class="moflow-title">MoFlow</span>: Self-Supervised Motion Learning for Dynamic Scene Reconstruction</title>


  <!-- Bootstrap -->
  <link rel="stylesheet" href="./static/css/bootstrap-4.4.1.css">

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">
  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/icon.png">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>

  <link rel="stylesheet" href="./static/css/dics.original.css">
  <script src="./static/js/event_handler.js"></script>
  <script src="./static/js/dics.original.js"></script>
  <script src="./static/js/video-comparison.js"></script>

  <style>
        .container {
            width: 80%; /* 或者根据需要设置为其他宽度 */
            margin: auto; /* 居中显示 */
        }

        .responsive-video {
            width: 45%; /* 视频宽度为容器的 100% */
            height: auto; /* 高度自动调整以保持宽高比 */
        }
    </style>

    <style>
        .fixed-size-video {
            width: 940px; /* 设置视频的固定宽度 */
            height: auto; /* 设置视频的固定高度 */
        }
    </style>

    <style>
        .fixed-size-video-small {
            width: auto; /* 设置视频的固定宽度 */
            height: 300px; /* 设置视频的固定高度 */
        }
        .rounded-corners {
    border-radius: 10px; /* 设置圆角的大小 */
  }
    </style>
    <style>
        .video-container {
            height: 600px; /* 统一设置视频高度 */
            width: auto;   /* 宽度自适应 */
        }
    </style>
    <style>
        .video-container {
            display: flex;         /* 使用 flexbox 布局 */
            justify-content: center; /* 水平居中 */
        }
        .video-container video {
            width: 30%;           /* 每个视频宽度占容器的30% */
            margin: 5px;          /* 视频之间的间隔 */
        }
    </style>

<style>
  .video-row {
    display: flex;
    justify-content: space-around; /* 可以改为 center 使得视频在行中居中 */
    margin-bottom: 20px; /* 行与行之间的间距 */
  }

  .fixed-size-video-small {
    width: 500px; /* 设置视频的固定宽度 */
            height: 200; /* 设置视频的固定高度 */
  }
  .fixed-size-video-small-two {
    width: 500px; /* 设置视频的固定宽度 */
            height: 200; /* 设置视频的固定高度 */
  }
  .fixed-size-video-small-three {
    width: 500px; /* 设置视频的固定宽度 */
            height: 200; /* 设置视频的固定高度 */
  }
  
  /* 可能需要调整的样式 */
  @media screen and (max-width: 768px) {
    /* 在较小屏幕上，让视频堆叠而不是并排 */
    .video-row {
      flex-direction: column;
    }
  }
</style>

<style>
        .text-container {
            max-width: 1070px; /* 限制文本容器的最大宽度 */
            margin: 0 auto;    /* 居中文本容器 */
            padding: 20px;     /* 为容器添加一些内边距 */
        }
        
    </style>

  <style>
    .moflow-title {
      background: linear-gradient(135deg, #ff7a7a 0%, #ff9a9a 100%);
      -webkit-background-clip: text;
      -webkit-text-fill-color: transparent;
      font-weight: 700;
      display: inline-block;
    }
  </style>

</head>
<body>

<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title"><span class="moflow-title">MoFlow</span>: Self-Supervised Motion Learning for Dynamic Scene Reconstruction</h1>
          <div class="is-size-5 publication-authors">
            <!-- Paper authors -->
            <span class="author-block">Anonymous Authors,</span>
          </div>
          <div class="is-size-5 publication-authors">
            <span class="author-block">Anonymous Institution</span><br>
            <span class="author-block"><b>Under Review at NeurIPS 2025</b></span><br>
          </div>

  
          
        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero teaser" style="padding-top: 1rem;">
  <div class="container is-max-desktop" style="margin: 2rem auto; max-width: 800px;">
    <div style="text-align: center; padding: 2.5rem; background: linear-gradient(135deg, #fff0f0 0%, #ffe6e6 100%); border-radius: 8px; box-shadow: 0 2px 15px rgba(255, 122, 122, 0.15);">
      <p style="font-family: 'Google Sans', sans-serif; font-size: 1.4rem; color: #ff7a7a; line-height: 1.6; margin: 0; font-weight: 500; font-style: italic;">
        Is it possible to unlock <b>4D dynamic scenes</b> purely from <b>2D observations</b>, without <b>external motion priors</b>?
      </p>
    </div>
  </div>

  <div class="columns is-centered has-text-centered" style="margin-top: 2rem;">
    <h2 class="title is-3">Motivation</h2>
  </div>

  <div class="container is-max-desktop">

    <div class="hero-body">
      <!-- <h2 class="title is-3">Framework</h2> -->
      <div class="content has-text-justified">

        <img src="./static/images/motivations.png"> 

        <p>
          <b>Motivation of <span class="moflow-title">MoFlow</span>.</b>
          (a) We start with a simple observation: 2D observations, such as the shifting balloon, are caused by 3D motion. 
          Accurate reconstructed 3D Motion should naturally align with these visible changes.
          (b) Unlike previous methods that use external motion priors to supervise 3D motion,
          we instead uses raw <b>video as motion supervision</b> through a self-supervised flow matching mechanism 
          to directly align predicted 3D motion projections with 2D frame differences.
        </p>

      </div>
    </div>
  </div>




  <br><br>

  <div class="columns is-centered has-text-centered">
      <h2 class="title is-3">Method Overview</h2>
  </div>
  <div class="container is-max-desktop">

        <div class="hero-body">
          <!-- <h2 class="title is-3">Framework</h2> -->
          <div class="content has-text-justified">

            <img src="./static/images/pipeline.png"> 

            <p>
              <b>Overview of <span class="moflow-title">MoFlow</span>. </b>
              We first build a separate scene representation tailored for self-supervised flow matching. 
              This begins with constructing a complete canonical space that includes both static and dynamic elements, 
              which are then disentangled using spatial and spatiotemporal feature planes. This detailed representation enables 
              applying targeted flow constraints—full flow supervises motion across the entire image, while camera flow focuses on static regions.
            </p>

          </div>
        </div>
      </div>

      <div class="container is-max-desktop">

        <div class="hero-body">
          <!-- <h2 class="title is-3">Framework</h2> -->
          <div class="content has-text-justified">

            <img src="./static/images/flow.png"> 

            <p>
              <b>Self-supervised flow matching mechanism.</b>
              (a) <b>Different Motion and Flow in the 4D Scene.</b> 
              Static areas move only due to camera motion (camera flow), while dynamic areas involve both camera and object motion (full flow). 
              Accurate motion learning requires region-specific flow supervision.
              (b) <b>Self-supervised flow matching.</b> We apply full flow to warp the entire image from state t<sub>1</sub> to state t<sub>2</sub> 
              and compare with the real observation, validating overall motion. Camera flow is used similarly but only on static regions, ensuring 
              their stability. Together, these provide a complementary self-supervised signal for 3D motion learning.
            </p>

          </div>
        </div>
      </div>



  <br><br>


  <div class="columns is-centered has-text-centered"></div>
    <h2 class="title is-3" style="text-align: center;">Reconstruction Results</h2>
  </div>



  <div class="container is-max-desktop">

      <div class="hero-body">
        <!-- <h2 class="title is-3">Framework</h2> -->
        <div class="content has-text-justified">

          <img src="./static/images/nvidia.png"> 

          <p>
            <b>Qualitative comparison on Nvidia Monocular dataset.</b> 
            Yellow boxes highlight zoomed-in regions for detail examination. Per-scene average PSNR values are provided.
          </p>

        </div>
      </div>
    </div>

    <div class="container is-max-desktop">

      <div class="hero-body">
        <!-- <h2 class="title is-3">Framework</h2> -->
        <div class="content has-text-justified">

          <img src="./static/images/hypernerf.png"> 

          <p>
            <b>Qualitative comparison on Nerfies-HyperNeRF dataset.</b> 
            Yellow boxes highlight zoomed-in regions for detail examination. Per-scene average PSNR values are provided.
          </p>

        </div>
      </div>
    </div>



  <br><br>


  <div class="columns is-centered has-text-centered"></div>
    <h2 class="title is-3" style="text-align: center;">More Visualization</h2>
  </div>



  <div class="container is-max-desktop">

      <div class="hero-body">
        <!-- <h2 class="title is-3">Framework</h2> -->
        <div class="content has-text-justified">

          <img src="./static/images/matching.png"> 

          <p>
            <b>Visualization of our self-supervised flow matching progress across training iterations</b>, 
            using the DynamicFace sequence from the Nvidia Monocular dataset. Each row shows results at different training iterations: 
            300 (top row), 1000 (second row), 5000 (third row), and 7000 (bottom row). The columns present: 
            <b>Left column (I<sub>1</sub>):</b> Ground truth image at time t<sub>1</sub>. 
            <b>Middle column (I<sub>1</sub><sup>warped</sup>):</b> Ground truth image from t<sub>1</sub> warped using our predicted flow field F, 
            demonstrating how content transforms to match the target frame. <b>Right column (I<sub>2</sub>):</b> Ground truth image at 
            time t<sub>2</sub>, serving as the reference for evaluating warping accuracy. As training progresses, I<sub>1</sub><sup>warped</sup> 
            increasingly aligns with I<sub>2</sub>, demonstrating that our self-supervised flow matching effectively learns to model dynamic scene 
            motion without requiring external motion priors.
          </p>

        </div>
      </div>
    </div>

    <div class="container is-max-desktop">

      <div class="hero-body">
        <!-- <h2 class="title is-3">Framework</h2> -->
        <div class="content has-text-justified">

          <img src="./static/images/nvidia_all.png"> 

          <p>
            <b>Qualitative comparison on all scenes from the Nvidia Monocular dataset.</b>
          </p>

        </div>
      </div>
    </div>

    <div class="container is-max-desktop">

      <div class="hero-body">
        <!-- <h2 class="title is-3">Framework</h2> -->
        <div class="content has-text-justified">

          <img src="./static/images/hypernerf_all.png"> 

          <p>
            <b>More qualitative comparisons from the Nerfies-HyperNeRF dataset.</b>
          </p>

        </div>
      </div>
    </div>

    <div class="container is-max-desktop">

      <div class="hero-body">
        <!-- <h2 class="title is-3">Framework</h2> -->
        <div class="content has-text-justified">

          <img src="./static/images/traj.png"> 

          <p>
            <b>Novel time synthesis results with trajectory visualization across different dynamic scenes.</b> 
            Each column shows a different dataset: Nvidia-Playground, Nvidia-Umbrella, Nvidia-Skating, Hypernerf-broom, and Hypernerf-banana. 
            The yellow boxes highlight dynamic/static regions with our tracked trajectories visualized using the Deform-GS approach. 
            Rows represent different timesteps with fixed camera positions, demonstrating how our method correctly models temporal scene evolution. 
            Note how static background elements remain perfectly stable across frames while dynamic components exhibit physically plausible motion paths. 
            The Playground scene (leftmost column) particularly demonstrates our method's capability to preserve fine structures like blue ribbons during motion, 
            which are typically challenging to reconstruct accurately.
          </p>

        </div>
      </div>
    </div>



  <br><br>


  <footer class="footer">
    <div class="container">
      <div class="columns is-centered">
        <div class="column is-8">
          <div class="content">
            <p>
              We thank the authors of <a href="https://nerfies.github.io/">Nerfies</a> that kindly open sourced the template of this website.
            </p>
            </p>
          </div>
        </div>
      </div>
    </div>
  </footer>

</body>
</html>
